{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oHrJSSHVSYiM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, out_size, batch_size, z_size):\n",
        "        super().__init__()\n",
        "        self.z_size = z_size\n",
        "        self.out_size = out_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "        self.linear = nn.Linear(z_size, out_size**2)\n",
        "\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_channels=1,\n",
        "            out_channels=batch_size,\n",
        "            kernel_size=out_size,\n",
        "            stride=out_size,\n",
        "            groups=1\n",
        "        )\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = x[:, None]\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, out_size, batch_size):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten(1,2)\n",
        "        self.linear_1 = nn.Linear(out_size*batch_size, 64)\n",
        "        self.linear_2 = nn.Linear(64, 128)\n",
        "        self.linear_3 = nn.Linear(128, 64)\n",
        "        self.linear_4 = nn.Linear(64, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.batch_norm = nn.BatchNorm1d(num_features=64)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.batch_norm(x)\n",
        "\n",
        "        x = self.linear_2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.linear_3(x)\n",
        "        x = self.leaky_relu(x)\n",
        "\n",
        "        x = self.linear_4(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QptVHzG4R4on"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# import draGAN_network\n",
        "\n",
        "\n",
        "class dragan_agent:\n",
        "    def __init__(self, model, value_function):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            model: an uninstantiate classifier object\n",
        "            value_function: a callable function that evaluates the performance.\n",
        "                            (the higher the better, and optimally in range [0,1])\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "\n",
        "        # define hyperparameters\n",
        "        self.z_size = 512\n",
        "        self.EPOCHS = 1750\n",
        "        self.Critic_EPOCHS = 2\n",
        "        self.batch_size = 16\n",
        "        self.max_memory_factor = 124\n",
        "        self.nr_samples_generated_factor = 1.7934693188444824#1.7935\n",
        "        self.G_LR = 0.0002660257499561004#0.000266\n",
        "        self.C_LR = 0.03628406973687752#0.036284\n",
        "        self.early_stopping_after = 921\n",
        "        self.value_function = value_function\n",
        "\n",
        "        self.MAX_LEN = self.batch_size * self.max_memory_factor\n",
        "\n",
        "    def _one_hot_to_label(self, matrix, enforce_balance=True):\n",
        "        if len(np.unique(np.argmax(matrix, axis=1))) == self.nr_labels or enforce_balance==False:\n",
        "            return np.argmax(matrix, axis=1)\n",
        "        else:\n",
        "            # check which label doesn't exists\n",
        "            missing_label = 1 - np.unique(np.argmax(matrix, axis=1))[0]\n",
        "\n",
        "            # assign a high number to the one with the highest prob\n",
        "            matrix[np.argmax(matrix[:, missing_label]), missing_label] = 999_999\n",
        "            return np.argmax(matrix, axis=1)\n",
        "\n",
        "\n",
        "    def train(self, X, y, verbose=1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            X (numpy array): The independent features\n",
        "            y (numpy array): The target features\n",
        "            verbose (boolean): Whether progress in printed\n",
        "        Returns:\n",
        "            A trained version of the classifier passed into draGAN during\n",
        "                instantiation.\n",
        "        \"\"\"\n",
        "        self.len_train = len(X)\n",
        "        self.nr_samples = int(self.len_train * self.nr_samples_generated_factor)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # set all random seeds for reproducibility\n",
        "        seed = 489\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        self.out_size = np.shape(X)[1]+2\n",
        "        self.nr_labels = len(np.unique(y))\n",
        "\n",
        "        X_train_discriminator = torch.zeros(\n",
        "            (self.MAX_LEN, self.nr_samples, self.out_size),\n",
        "            device=device,\n",
        "            dtype=torch.float\n",
        "        )\n",
        "        y_train_discriminator = torch.zeros(\n",
        "            (self.MAX_LEN, 1, 1),\n",
        "            device=device,\n",
        "            dtype=torch.float\n",
        "        )\n",
        "        train_discriminator_populated = np.zeros(\n",
        "            (self.MAX_LEN),\n",
        "            dtype=np.float\n",
        "        )\n",
        "\n",
        "\n",
        "        # instantiate networks\n",
        "        G = Generator(\n",
        "            z_size=self.z_size,\n",
        "            out_size=self.out_size,\n",
        "            batch_size=self.nr_samples\n",
        "        )\n",
        "        G.to(device)\n",
        "\n",
        "        C = Critic(\n",
        "            out_size=self.out_size,\n",
        "            batch_size=self.nr_samples\n",
        "        )\n",
        "        C.to(device)\n",
        "\n",
        "        # Define loss function\n",
        "        loss_G = nn.MSELoss()\n",
        "        loss_C = nn.MSELoss()\n",
        "\n",
        "        # Define optimizer\n",
        "        optimizer_G = optim.RMSprop(G.parameters(), self.G_LR)\n",
        "        optimizer_C = optim.Adam(C.parameters(), self.C_LR)\n",
        "\n",
        "        G.eval()\n",
        "        C.eval()\n",
        "\n",
        "\n",
        "        same_for = 0\n",
        "        best_train_score = 0\n",
        "        for epoch in range(self.EPOCHS):\n",
        "            # generate the Gaussian noies vector\n",
        "            z = torch.autograd.Variable(torch.normal(\n",
        "            torch.Tensor([0]*self.z_size*self.batch_size),\n",
        "            torch.Tensor([.1]*self.z_size*self.batch_size)\n",
        "            )).view(self.batch_size, self.z_size)\n",
        "            z = z.to(device)\n",
        "\n",
        "            # alternate between train and eval\n",
        "            if not epoch%3:\n",
        "                G.eval()\n",
        "                C.eval()\n",
        "            else:\n",
        "                G.train()\n",
        "                C.train()\n",
        "\n",
        "            # generate training data using the Gaussian Noise\n",
        "            optimizer_G.zero_grad()\n",
        "            train_data = G(z.clone().detach())\n",
        "\n",
        "\n",
        "            # assess the auc of the training data\n",
        "            train_data_batch_numpy = train_data.cpu().clone().detach().numpy()\n",
        "            for i in range(len(train_data)):\n",
        "                train_data_numpy = train_data_batch_numpy[i]\n",
        "\n",
        "                X_train_d = train_data_numpy[:, :-self.nr_labels]\n",
        "                y_train_d = self._one_hot_to_label(train_data_numpy[:, -self.nr_labels:])\n",
        "\n",
        "                # instantiate and train the actual classifier on the generated data\n",
        "                model = self.model()\n",
        "                model.fit(\n",
        "                    X_train_d,\n",
        "                    y_train_d\n",
        "                )\n",
        "\n",
        "                # evalute the performance of the classifier on the passed data\n",
        "                # with the evaluation metric provided.\n",
        "                y_val = model.predict_proba(X)[:,1]\n",
        "                val_score = self.value_function(y, y_val)\n",
        "\n",
        "                # keep track of the best performing batch for final training\n",
        "                if val_score >= best_train_score:\n",
        "                    if val_score > best_train_score:\n",
        "                        same_for = 0\n",
        "                    best_train_score = val_score\n",
        "                    X_train_gen = X_train_d.copy()\n",
        "                    y_train_gen = y_train_d.copy()\n",
        "\n",
        "                # check if early stopping condition is triggered\n",
        "                same_for += 1\n",
        "                if same_for > self.early_stopping_after*len(train_data):\n",
        "                    break\n",
        "\n",
        "\n",
        "                # append to discriminator training data\n",
        "                rndm_idx = np.random.choice(\n",
        "                    np.arange(len(X_train_discriminator)),\n",
        "                    replace=False,\n",
        "                    size=1\n",
        "                )\n",
        "                X_train_discriminator[rndm_idx] = torch.tensor(\n",
        "                    train_data_numpy.copy(),\n",
        "                    device=device,\n",
        "                    dtype=torch.float\n",
        "                )\n",
        "                y_train_discriminator[rndm_idx] = val_score\n",
        "                train_discriminator_populated[rndm_idx] = 1\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"{epoch} / {self.EPOCHS}\\t\"+\\\n",
        "                    f\"auc: {val_score:.4f}\\t\"+\\\n",
        "                    f\"best_train_score: {best_train_score:.4f}\", end=\"\\r\")\n",
        "\n",
        "\n",
        "            # re-train the Critic\n",
        "            for c_epoch in range(self.Critic_EPOCHS):\n",
        "                optimizer_C.zero_grad()\n",
        "                use_idx = np.where(train_discriminator_populated==1)\n",
        "                y_pred = C(\n",
        "                    X_train_discriminator[use_idx],\n",
        "                )\n",
        "                loss_c = loss_C(\n",
        "                    y_pred,\n",
        "                    y_train_discriminator[use_idx][:,:,0],\n",
        "                )\n",
        "                loss_c.backward()\n",
        "                optimizer_C.step()\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "            # train the generator\n",
        "            loss = loss_G(\n",
        "                C(train_data),\n",
        "                torch.ones((len(train_data), 1)).to(device)\n",
        "            )\n",
        "            loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "        # instantiate the passed classifier one more time and train it on the\n",
        "        # most promising set of generated data\n",
        "        self.model = self.model()\n",
        "        self.model.fit(X_train_gen, y_train_gen)\n",
        "        clf = DecisionTreeClassifier(random_state=1234, max_depth=4),\n",
        "        clf.fit(X_train_gen, y_train_gen),\n",
        "        fig = plt.figure(figsize=(25,20)),\n",
        "        tree.plot_tree(clf,\n",
        "        feature_names=X_train_gen,\n",
        "        class_names=y_train_gen,\n",
        "        filled=True),\n",
        "       # Add other visualization plots like feature importance and shapley plots here\\n\"\n",
        "        return self.model\n",
        "\n",
        "    def predict(self, X):\n",
        "        # predict the probabilities for y-labels based on the passed X-data\n",
        "        return self.model.predict_proba(X)[:,1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2FcRCoeRX8N",
        "outputId": "d6a0b41e-29a1-4338-c888-39b025fc36e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imbalanced_databases\n",
            "  Using cached imbalanced_databases-0.1.1-py3-none-any.whl (4.3 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imbalanced_databases) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from imbalanced_databases) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imbalanced_databases) (1.10.1)\n",
            "Collecting sklearn (from imbalanced_databases)\n",
            "  Using cached sklearn-0.0.post7.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->imbalanced_databases) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->imbalanced_databases) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->imbalanced_databases) (1.16.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post7-py3-none-any.whl size=2951 sha256=b041f64d442657a848d28611e4ac8d57c3d9332b4e4eaf787dd17ae36f25fa17\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/9c/85/72901eb50bc4bc6e3b2629378d172384ea3dfd19759c77fd2c\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn, imbalanced_databases\n",
            "Successfully installed imbalanced_databases-0.1.1 sklearn-0.0.post7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f4f6c11eddb>:84: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dtype=np.float\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import random, torch\n",
        "!pip install imbalanced_databases\n",
        "import imbalanced_databases as imdb\n",
        "\n",
        "# import draGAN\n",
        "\n",
        "# set seed\n",
        "seed = 489\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# load data\n",
        "data_dict = imdb.load_abalone_17_vs_7_8_9_10(encode=True)\n",
        "\n",
        "# train test split\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "for train_index, test_index in skf.split(data_dict[\"data\"], data_dict[\"target\"]):\n",
        "    X_train, y_train = data_dict[\"data\"][train_index], data_dict[\"target\"][train_index]\n",
        "    X_test, y_test = data_dict[\"data\"][test_index], data_dict[\"target\"][test_index]\n",
        "\n",
        "    # instantiate draGAN\n",
        "    model = dragan_agent(\n",
        "        model=LogisticRegression,\n",
        "        value_function=roc_auc_score\n",
        "    )\n",
        "    # train draGAN\n",
        "    model.train(\n",
        "        X=X_train,\n",
        "        y=y_train\n",
        "    )\n",
        "\n",
        "    # test draGAN\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"\\nTest AUC-Score: {roc_auc_score(y_test, y_pred)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}